{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Q1 \n",
        "    Take any YouTube videos link and your task is to extract the comments from that videos and store it in a csv file and then you need define what is most demanding topic in that videos comment section."
      ],
      "metadata": {
        "id": "0IcNt0uYau6c"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kf3vuWhZapFl",
        "outputId": "14594327-e140-4e44-bff2-d56e58c05253"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting youtube-dl\n",
            "  Downloading youtube_dl-2021.12.17-py2.py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: youtube-dl\n",
            "Successfully installed youtube-dl-2021.12.17\n"
          ]
        }
      ],
      "source": [
        "! pip install youtube-dl\n",
        "# ! pip install pandas\n",
        "# ! pip install nltk\n",
        "# ! pip install sklearn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install pytube\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VA8Xx7iN0QIY",
        "outputId": "f3da036b-a0ad-47b7-8b50-f4b71d4d6cc6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pytube\n",
            "  Downloading pytube-15.0.0-py3-none-any.whl (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pytube\n",
            "Successfully installed pytube-15.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the required libraries:\n",
        "import youtube_dl\n",
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.decomposition import PCA\n"
      ],
      "metadata": {
        "id": "WVZ5dAqqcoI8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up the function to extract comments from the YouTube video:\n",
        "\n",
        "# def extract_comments(video_url):\n",
        "#     ydl_opts = {'extract_flat': 'comments'}\n",
        "#     with youtube_dl.YoutubeDL(ydl_opts) as ydl:\n",
        "#         info = ydl.extract_info(video_url, download=False)\n",
        "#         comments = info['comments']\n",
        "#     return comments\n",
        "\n",
        "from pytube import YouTube\n",
        "\n",
        "def extract_comments(video_url):\n",
        "    comments = []\n",
        "    try:\n",
        "        youtube = YouTube(video_url)\n",
        "        comments = youtube.comments\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting comments: {e}\")\n",
        "    return comments\n"
      ],
      "metadata": {
        "id": "EAQSxmATyj7x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up the function to preprocess the comments:\n",
        "nltk.download('stopwords')\n",
        "\n",
        "stopwords = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess_text(comment):\n",
        "    # Remove URLs\n",
        "    comment = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", comment, flags=re.MULTILINE)\n",
        "\n",
        "    # Remove special characters and numbers\n",
        "    comment = re.sub('[^a-zA-Z]', ' ', comment)\n",
        "\n",
        "    # Convert to lowercase and remove stopwords\n",
        "    comment = comment.lower()\n",
        "    comment = \" \".join(word for word in comment.split() if word not in stopwords)\n",
        "\n",
        "    return comment\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "szGuvsGpyqbW",
        "outputId": "bc4377c9-a176-4310-f2a0-ee398c0b71db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up the function to identify the most demanding topic:\n",
        "def identify_demanding_topic(comments):\n",
        "\n",
        "    # Preprocess the comments\n",
        "    preprocessed_comments = [preprocess_text(comment) for comment in comments]\n",
        "\n",
        "    # Check if any meaningful words are present in the comments\n",
        "    filtered_comments = [comment for comment in preprocessed_comments if len(comment.split()) > 0]\n",
        "\n",
        "    if len(filtered_comments) == 0:\n",
        "        print(\"No meaningful words found in the comments.\")\n",
        "    else:\n",
        "        # TF-IDF vectorization\n",
        "        vectorizer = TfidfVectorizer()\n",
        "        comment_vectors = vectorizer.fit_transform(filtered_comments)\n",
        "        # Preprocess the comments\n",
        "        preprocessed_comments = [preprocess_text(comment) for comment in comments]\n",
        "\n",
        "        # TF-IDF vectorization\n",
        "        vectorizer = TfidfVectorizer()\n",
        "        comment_vectors = vectorizer.fit_transform(preprocessed_comments)\n",
        "\n",
        "        # Dimensionality reduction with PCA\n",
        "        pca = PCA(n_components=2)\n",
        "        comment_vectors_pca = pca.fit_transform(comment_vectors.toarray())\n",
        "\n",
        "        # K-means clustering\n",
        "        num_clusters = 5\n",
        "        kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
        "        cluster_labels = kmeans.fit_predict(comment_vectors)\n",
        "\n",
        "        # Analyze clusters and identify most demanding topic\n",
        "        cluster_comments = [[] for _ in range(num_clusters)]\n",
        "        for i, comment in enumerate(comments):\n",
        "            cluster_comments[cluster_labels[i]].append(comment)\n",
        "\n",
        "        most_demanding_topic = None\n",
        "        max_cluster_size = 0\n",
        "\n",
        "        for i, comments_in_cluster in enumerate(cluster_comments):\n",
        "            cluster_size = len(comments_in_cluster)\n",
        "            if cluster_size > max_cluster_size:\n",
        "                max_cluster_size = cluster_size\n",
        "                most_demanding_topic = comments_in_cluster\n",
        "\n",
        "        return most_demanding_topic\n"
      ],
      "metadata": {
        "id": "lxKSzteCyx8S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install -U youtube-dl"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-o_wJzNHzYtQ",
        "outputId": "3505da94-09b4-4a57-de06-baa03a25a25f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: youtube-dl in /usr/local/lib/python3.10/dist-packages (2021.12.17)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Provide the YouTube video URL and run the code:\n",
        "video_url = \"https://www.youtube.com/watch?v=HlKpcWN3Xjg&ab_channel=KrishNaik\"\n",
        "\n",
        "# Extract comments from the YouTube video\n",
        "comments = extract_comments(video_url)\n",
        "\n",
        "# Save comments to a CSV file\n",
        "comments_df = pd.DataFrame(comments, columns=['Comment'])\n",
        "comments_df.to_csv('comments.csv', index=False)\n",
        "\n",
        "# Identify the most demanding topic\n",
        "most_demanding_topic = identify_demanding_topic(comments)\n",
        "print(\"Most Demanding Topic:\")\n",
        "print(most_demanding_topic)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1WmC3b2Uy2XG",
        "outputId": "02c226b6-3838-46da-d1cf-28f0b1d559ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error extracting comments: 'YouTube' object has no attribute 'comments'\n",
            "No meaningful words found in the comments.\n",
            "Most Demanding Topic:\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iquLrCOKzCOS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "    Q-2. Take any pdf and your task is to extract the text from that pdf and store it in a csv file and then you need to find the most repeated word in that pdf.\n",
        "    Q-3. from question 2, As you got the CSV and now you need perform key word extraction from that csv file and do the Topic modeling\n",
        "    Q-4. Take any text file and now your task is to Text Summarization without using hugging transformer library\n",
        "    Q-5. Now you need build your own language detection with the fast Text model by Facebook and\n",
        "    Q-6. Generate research papers titles using Bert model and containerize the application and push to public docker hub\n",
        "    Q-7. Now you need to build your own chatbot using the seq2seq model of Amazon website by scrape the website and containerize the application and push to public docker hub\n",
        "    Q-8. Take a any own dataset and build a knowledge bot using Llama model.\n",
        "    Q-9. Using wisher you need transcribe any audio file and then you need to convert that audio file into text file and now convert that text file into audio file of different language.\n",
        "    Q-10. Build a whole End- End api and deploy it on Heroku /railways so the task is that you need build a Auto-Correction of text using NLP\n",
        "    Note: only Jupyter notebook is not allowed from 5th question"
      ],
      "metadata": {
        "id": "bDqyvjtY5EBz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q 2\n",
        "    Take any pdf and your task is to extract the text from that pdf and store it in a csv file and then you need to find the most repeated word in that pdf."
      ],
      "metadata": {
        "id": "7_eBFIZFiMcO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ! pip install PyPDF2\n"
      ],
      "metadata": {
        "id": "sBAgZDV1kJxz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import files\n",
        "# files.upload()"
      ],
      "metadata": {
        "id": "cLUeVUEVjZ2X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import PyPDF2\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "import re\n",
        "\n",
        "def extract_text_from_pdf(file_path):\n",
        "    with open(file_path, 'rb') as file:\n",
        "        pdf_reader = PyPDF2.PdfReader(file)\n",
        "        text = ''\n",
        "        for page in pdf_reader.pages:\n",
        "            text += page.extract_text()\n",
        "    return text\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Remove special characters and numbers\n",
        "    # text = re.sub('[^a-zA-Z]', ' ', text)\n",
        "\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "    # print(text)\n",
        "    return text\n",
        "\n",
        "def find_most_repeated_word(text):\n",
        "    # Preprocess the text\n",
        "    preprocessed_text = preprocess_text(text)\n",
        "\n",
        "    # Split the text into words\n",
        "    words = preprocessed_text.split()\n",
        "\n",
        "    # Count word occurrences\n",
        "    word_counts = Counter(words)\n",
        "\n",
        "    # Find the most repeated word\n",
        "    most_repeated_word = word_counts.most_common(1)[0][0]\n",
        "\n",
        "    return most_repeated_word\n",
        "\n",
        "pdf_file_path = \"/content/Data Science.pdf\"\n",
        "\n",
        "# Extract text from the PDF file\n",
        "text = extract_text_from_pdf(pdf_file_path)\n",
        "\n",
        "# Find the most repeated word\n",
        "most_repeated_word = find_most_repeated_word(text)\n",
        "\n",
        "# Save the text to a CSV file\n",
        "df = pd.DataFrame({'Text': [text]})\n",
        "df.to_csv('extracted_text.csv', index=False)\n",
        "\n",
        "# Print the most repeated word\n",
        "print(\"Most Repeated Word:\", most_repeated_word)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uyq7JJG25Jav",
        "outputId": "a1b381ff-b045-4802-979a-32df6e4e02fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most Repeated Word: the\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q-3. \n",
        "    from question 2, As you got the CSV and now you need perform key word extraction from that csv file and do the Topic modeling\n"
      ],
      "metadata": {
        "id": "QBIaTWPwlrFe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the necessary libraries:\n",
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from gensim import models, corpora\n",
        "\n",
        "csv_file_path = \"/content/extracted_text.csv\"\n",
        "df = pd.read_csv(csv_file_path)\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "stopwords = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Remove special characters and numbers\n",
        "    text = re.sub('[^a-zA-Z]', ' ', text)\n",
        "\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Tokenize the text\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Remove stopwords\n",
        "    filtered_tokens = [token for token in tokens if token not in stopwords]\n",
        "\n",
        "    return filtered_tokens\n",
        "\n",
        "# Preprocess the text and create a list of tokenized documents:\n",
        "tokenized_docs = [preprocess_text(text) for text in df['Text']]\n",
        "\n",
        "# Create a dictionary and corpus for topic modeling:\n",
        "# Create a dictionary\n",
        "dictionary = corpora.Dictionary(tokenized_docs)\n",
        "\n",
        "# Create a corpus\n",
        "corpus = [dictionary.doc2bow(tokens) for tokens in tokenized_docs]\n",
        "# Perform topic modeling using the Latent Dirichlet Allocation (LDA) algorithm:\n",
        "# Train the LDA model\n",
        "num_topics = 5  # Set the number of topics\n",
        "lda_model = models.LdaModel(corpus, num_topics=num_topics, id2word=dictionary, passes=10)\n",
        "\n",
        "# Get the topics and their corresponding keywords\n",
        "topics = lda_model.print_topics(num_words=5)\n",
        "for topic in topics:\n",
        "    print(topic)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nWv1HzDykQuQ",
        "outputId": "398fc018-deae-4ca0-8bc3-3e0e22fae0c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(0, '0.029*\"q\" + 0.022*\"e\" + 0.017*\"question\" + 0.014*\"n\" + 0.014*\"r\"')\n",
            "(1, '0.002*\"q\" + 0.002*\"e\" + 0.002*\"question\" + 0.002*\"n\" + 0.002*\"note\"')\n",
            "(2, '0.002*\"q\" + 0.002*\"question\" + 0.002*\"e\" + 0.002*\"note\" + 0.002*\"b\"')\n",
            "(3, '0.002*\"q\" + 0.002*\"e\" + 0.002*\"n\" + 0.002*\"note\" + 0.002*\"c\"')\n",
            "(4, '0.002*\"q\" + 0.002*\"e\" + 0.002*\"question\" + 0.002*\"r\" + 0.002*\"note\"')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q-4. \n",
        "    Take any text file and now your task is to Text Summarization without using hugging transformer library"
      ],
      "metadata": {
        "id": "rNeOvEaYsKdW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yAs_s1ThSG8l",
        "outputId": "43520bdb-cbbd-421e-bd3e-781b891fedbf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.cluster.util import cosine_distance\n",
        "import numpy as np\n",
        "\n",
        "# Sample text\n",
        "text = '''\n",
        "Q-1. Take any YouTube videos link and your task is to extract the comments from that videos and store it in a csv file and then you need define what is most demanding topic in that videos comment section.\n",
        "Q-2. Take any pdf and your task is to extract the text from that pdf and store it in a csv file and then you need to find the most repeated word in that pdf.\n",
        "Q-3. from question 2, As you got the CSV and now you need perform key word extraction from that csv file and do the Topic modeling\n",
        "Q-4. Take any text file and now your task is to Text Summarization without using hugging transformer library\n",
        "Q-5. Now you need build your own language detection with the fast Text model by Facebook and\n",
        "Q-6. Generate research papers titles using Bert model and containerize the application and push to public docker hub\n",
        "Q-7. Now you need to build your own chatbot using the seq2seq model of Amazon website by scrape the website and containerize the application and push to public docker hub\n",
        "Q-8. Take a any own dataset and build a knowledge bot using Llama model.\n",
        "Q-9. Using wisher you need transcribe any audio file and then you need to convert that audio file into text file and now convert that text file into audio file of different language.\n",
        "Q-10. Build a whole End- End api and deploy it on Heroku /railways so the task is that you need build a Auto-Correction of text using NLP\n",
        "Note: only Jupyter notebook is not allowed from 5th question\n",
        "'''\n",
        "\n",
        "# Tokenize the text into sentences\n",
        "sentences = sent_tokenize(text)\n",
        "\n",
        "# Create a list of stop words\n",
        "stop_words = stopwords.words('english')\n",
        "\n",
        "# Create a similarity matrix\n",
        "def sentence_similarity(sentence1, sentence2, stopwords=None):\n",
        "    if stopwords is None:\n",
        "        stopwords = []\n",
        "    \n",
        "    # Convert sentences to lowercase\n",
        "    sentence1 = sentence1.lower()\n",
        "    sentence2 = sentence2.lower()\n",
        "\n",
        "    # Tokenize sentences into words\n",
        "    words1 = nltk.word_tokenize(sentence1)\n",
        "    words2 = nltk.word_tokenize(sentence2)\n",
        "\n",
        "    # Remove stopwords\n",
        "    words1 = [word for word in words1 if word not in stopwords]\n",
        "    words2 = [word for word in words2 if word not in stopwords]\n",
        "\n",
        "    # Calculate the similarity between the sentences based on word overlap (using cosine distance)\n",
        "    return 1 - cosine_distance(np.array(list(nltk.FreqDist(words1).values())), np.array(list(nltk.FreqDist(words2).values())))\n",
        "\n",
        "# Create a similarity matrix\n",
        "def build_similarity_matrix(sentences, stopwords=None):\n",
        "    # Initialize the matrix with zeros\n",
        "    similarity_matrix = np.zeros((len(sentences), len(sentences)))\n",
        "\n",
        "    for i in range(len(sentences)):\n",
        "        for j in range(len(sentences)):\n",
        "            if i != j:\n",
        "                similarity_matrix[i][j] = sentence_similarity(sentences[i], sentences[j], stopwords)\n",
        "\n",
        "    return similarity_matrix\n",
        "\n",
        "# Generate the summary\n",
        "def generate_summary(text, top_n=5):\n",
        "    # Tokenize the text into sentences\n",
        "    sentences = sent_tokenize(text)\n",
        "\n",
        "    # Create the similarity matrix\n",
        "    similarity_matrix = build_similarity_matrix(sentences, stop_words)\n",
        "\n",
        "    # Calculate sentence scores based on similarity matrix\n",
        "    scores = np.sum(similarity_matrix, axis=1)\n",
        "\n",
        "    # Sort the sentences based on scores in descending order\n",
        "    ranked_sentences = [sentence for _, sentence in sorted(zip(scores, sentences), reverse=True)]\n",
        "\n",
        "    # Select the top 'top_n' sentences as the summary\n",
        "    summary = ranked_sentences[:top_n]\n",
        "\n",
        "    # Join the sentences to form the summary text\n",
        "    summary_text = ' '.join(summary)\n",
        "\n",
        "    return summary_text\n",
        "\n",
        "# Generate a summary of the text\n",
        "summary = generate_summary(text, top_n=3)\n",
        "\n",
        "# Print the summary\n",
        "print(summary)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 294
        },
        "id": "nZBMoowbnaVl",
        "outputId": "dc709fc8-2fe9-4384-92e9-2b6887dc7c5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-6d7c76cc1a82>\u001b[0m in \u001b[0;36m<cell line: 81>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;31m# Generate a summary of the text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m \u001b[0msummary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_n\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;31m# Print the summary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-6d7c76cc1a82>\u001b[0m in \u001b[0;36mgenerate_summary\u001b[0;34m(text, top_n)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;31m# Create the similarity matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m     \u001b[0msimilarity_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_similarity_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;31m# Calculate sentence scores based on similarity matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-6d7c76cc1a82>\u001b[0m in \u001b[0;36mbuild_similarity_matrix\u001b[0;34m(sentences, stopwords)\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m                 \u001b[0msimilarity_matrix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msentence_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msimilarity_matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-6d7c76cc1a82>\u001b[0m in \u001b[0;36msentence_similarity\u001b[0;34m(sentence1, sentence2, stopwords)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;31m# Calculate the similarity between the sentences based on word overlap (using cosine distance)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mcosine_distance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFreqDist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFreqDist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;31m# Create a similarity matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/cluster/util.py\u001b[0m in \u001b[0;36mcosine_distance\u001b[0;34m(u, v)\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0mequal\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m|\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m|\u001b[0m\u001b[0;34m|\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m|\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m     \"\"\"\n\u001b[0;32m--> 130\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/core/overrides.py\u001b[0m in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: shapes (15,) and (13,) not aligned: 15 (dim 0) != 13 (dim 0)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q 5\n",
        "    build your own language detection with the fast Text model by Facebook"
      ],
      "metadata": {
        "id": "5jZQJFcPT_4E"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rN5hSy7Ctf9U"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}